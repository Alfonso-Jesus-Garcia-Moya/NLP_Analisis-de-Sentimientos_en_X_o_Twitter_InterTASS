{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRuPwO5Hhv8hZijORi11js",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alfonso-Jesus-Garcia-Moya/NLP_Analisis-de-Sentimientos_en_X_o_Twitter_InterTASS/blob/main/InterTASS_y_Emo_(An%C3%A1lisis_de_Sentimientos).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fase 1. Preparaci√≥n y exploraci√≥n de datos\n",
        "1.\tCargar los datasets (InterTASS y EmoEvent) en formato CSV y XML.\n",
        "2.\tExplorar la distribuci√≥n de clases (porcentaje de tweets positivos, negativos, etc.).\n",
        "3.\tVisualizar ejemplos de cada categor√≠a.\n"
      ],
      "metadata": {
        "id": "DdC313DXmzZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FASE 1 MAESTRA: INGESTI√ìN Y EDA SIMULT√ÅNEO (SENTIMIENTOS + EMOCIONES)\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Instalaci√≥n de librer√≠as (solo si no est√°n)\n",
        "try:\n",
        "    import xmltodict\n",
        "except ImportError:\n",
        "    !pip install -q xmltodict\n",
        "    import xmltodict\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Configuraci√≥n visual global\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 5)\n",
        "\n",
        "# ==============================================================================\n",
        "# A. DEFINICI√ìN DE RUTAS Y FUNCIONES DE CARGA ESPECIALIZADAS\n",
        "# ==============================================================================\n",
        "\n",
        "# Rutas de tus archivos (Ajusta si cambian de nombre)\n",
        "ruta_intertass = '/content/data/general-train-tagged.xml'  # Sentimientos\n",
        "ruta_emoevent = '/content/data/emoevent_es.csv'            # Emociones\n",
        "\n",
        "# Funci√≥n 1: Carga XML Robusta (Para InterTASS - Sentimientos)\n",
        "def cargar_sentimientos_xml(ruta):\n",
        "    print(f\"üîÑ Procesando Sentimientos (XML) desde: {ruta}...\")\n",
        "    if not os.path.exists(ruta): return None, \"Archivo no encontrado\"\n",
        "\n",
        "    with open(ruta, 'r', encoding='utf-8') as f:\n",
        "        xml_content = f.read()\n",
        "\n",
        "    data = xmltodict.parse(xml_content)\n",
        "\n",
        "    # Navegaci√≥n flexible por la estructura XML\n",
        "    if 'tweets' in data: tweets = data['tweets']['tweet']\n",
        "    elif 'intertass' in data: tweets = data['intertass']['tweet']\n",
        "    else: tweets = data[list(data.keys())[0]]['tweet']\n",
        "\n",
        "    rows = []\n",
        "    for t in tweets:\n",
        "        try:\n",
        "            # Extracci√≥n de Sentimiento (Polaridad)\n",
        "            sent_node = t['sentiments']['polarity']\n",
        "            label = None\n",
        "\n",
        "            # L√≥gica para encontrar la etiqueta global (sin 'entity')\n",
        "            if isinstance(sent_node, list):\n",
        "                for item in sent_node:\n",
        "                    if 'entity' not in item:\n",
        "                        label = item['value']\n",
        "                        break\n",
        "                if not label: label = sent_node[0]['value']\n",
        "            elif isinstance(sent_node, dict):\n",
        "                label = sent_node['value']\n",
        "\n",
        "            # Normalizaci√≥n (P+ -> P, N+ -> N)\n",
        "            if label == 'P+': label = 'P'\n",
        "            if label == 'N+': label = 'N'\n",
        "\n",
        "            # Filtrado estricto\n",
        "            if label in ['P', 'N', 'NEU', 'NONE']:\n",
        "                rows.append({'content': t['content'], 'label': label})\n",
        "        except: continue\n",
        "\n",
        "    return pd.DataFrame(rows), \"OK\"\n",
        "\n",
        "# Funci√≥n 2: Carga CSV Robusta (Para EmoEvent - Emociones)\n",
        "def cargar_emociones_csv(ruta):\n",
        "    print(f\"üîÑ Procesando Emociones (CSV) desde: {ruta}...\")\n",
        "    if not os.path.exists(ruta): return None, \"Archivo no encontrado\"\n",
        "\n",
        "    try:\n",
        "        # Intentamos leer con tabulador primero\n",
        "        df = pd.read_csv(ruta, sep='\\t')\n",
        "        if len(df.columns) < 2: df = pd.read_csv(ruta, sep=',') # Fallback a coma\n",
        "\n",
        "        # Renombrar columnas para estandarizar (buscamos 'tweet' y 'emotion')\n",
        "        df.rename(columns={'tweet': 'content', 'emotion': 'label'}, inplace=True)\n",
        "\n",
        "        # Mapeo Ingl√©s -> Espa√±ol (Tus 7 clases objetivo)\n",
        "        mapa = {\n",
        "            'joy': 'Alegr√≠a', 'sadness': 'Tristeza', 'anger': 'Ira',\n",
        "            'disgust': 'Asco', 'fear': 'Miedo', 'surprise': 'Sorpresa',\n",
        "            'others': 'Neutral', 'neutral': 'Neutral' # 'others' suele ser Neutral\n",
        "        }\n",
        "        df['label'] = df['label'].map(mapa)\n",
        "\n",
        "        # Filtrar nulos (etiquetas no mapeadas)\n",
        "        df_clean = df.dropna(subset=['label'])[['content', 'label']]\n",
        "        return df_clean, \"OK\"\n",
        "\n",
        "    except Exception as e: return None, str(e)\n",
        "\n",
        "# ==============================================================================\n",
        "# B. EJECUCI√ìN SIMULT√ÅNEA DE CARGAS\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Cargar Sentimientos (InterTASS)\n",
        "df_sent, status_sent = cargar_sentimientos_xml(ruta_intertass)\n",
        "\n",
        "# 2. Cargar Emociones (EmoEvent)\n",
        "df_emo, status_emo = cargar_emociones_csv(ruta_emoevent)\n",
        "\n",
        "# ==============================================================================\n",
        "# C. VISUALIZACI√ìN COMPARATIVA (DASHBOARD)\n",
        "# ==============================================================================\n",
        "\n",
        "if (df_sent is not None) and (df_emo is not None):\n",
        "    print(\"\\n‚úÖ AMBOS DATASETS CARGADOS CORRECTAMENTE.\")\n",
        "    print(f\"   - Sentimientos (InterTASS): {len(df_sent)} tweets\")\n",
        "    print(f\"   - Emociones (EmoEvent):     {len(df_emo)} tweets\")\n",
        "\n",
        "    # Crear dashboard de 2 gr√°ficas\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Gr√°fica 1: Sentimientos\n",
        "    colores_sent = {'P': '#2ecc71', 'N': '#e74c3c', 'NEU': '#95a5a6', 'NONE': '#3498db'}\n",
        "    sns.countplot(x='label', data=df_sent, order=['P', 'N', 'NEU', 'NONE'], palette=colores_sent, ax=ax[0])\n",
        "    ax[0].set_title('Distribuci√≥n de Sentimientos (InterTASS)', fontsize=12, fontweight='bold')\n",
        "    ax[0].set_ylabel('Tweets')\n",
        "\n",
        "    # Gr√°fica 2: Emociones\n",
        "    sns.countplot(y='label', data=df_emo, palette='magma',\n",
        "                  order=df_emo['label'].value_counts().index, ax=ax[1])\n",
        "    ax[1].set_title('Distribuci√≥n de Emociones (EmoEvent)', fontsize=12, fontweight='bold')\n",
        "    ax[1].set_xlabel('Tweets')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Muestreo r√°pido de datos\n",
        "    print(\"\\nüîé VISTA PREVIA DE DATOS UNIFICADOS:\")\n",
        "    print(\"--- InterTASS (Sentimiento) ---\")\n",
        "    print(df_sent.head(3))\n",
        "    print(\"\\n--- EmoEvent (Emoci√≥n) ---\")\n",
        "    print(df_emo.head(3))\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå ERROR CR√çTICO:\")\n",
        "    print(f\"   - Estado Sentimientos: {status_sent}\")\n",
        "    print(f\"   - Estado Emociones:    {status_emo}\")"
      ],
      "metadata": {
        "id": "zo6nroWxC-AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fase 2. Preprocesamiento de texto\n",
        "‚Ä¢\tEliminaci√≥n de URLs, menciones y hashtags.\n",
        "\n",
        "‚Ä¢\tConversi√≥n a min√∫sculas y eliminaci√≥n de signos de puntuaci√≥n.\n",
        "\n",
        "‚Ä¢\tTokenizaci√≥n y eliminaci√≥n de stopwords.\n",
        "\n",
        "‚Ä¢\tRepresentaci√≥n de los textos mediante:\n",
        "\n",
        "o\tTF-IDF para modelos cl√°sicos (Logistic Regression).\n",
        "\n",
        "o\tEmbeddings de BETO (BERT en espa√±ol) para modelos de deep learning.\n"
      ],
      "metadata": {
        "id": "wTvhIbWHm5dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FASE 2: PREPROCESAMIENTO PROFUNDO Y VECTORIZACI√ìN SIMULT√ÅNEA\n",
        "# ==============================================================================\n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 1. Descarga de recursos NLTK (Anti-fallos)\n",
        "print(\"üì• Cargando recursos ling√º√≠sticos...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "stop_words_es = set(stopwords.words('spanish'))\n",
        "\n",
        "# ==============================================================================\n",
        "# A. FUNCI√ìN DE LIMPIEZA MAESTRA (AQU√ç EST√Å LA CORRECCI√ìN QUE PEDISTE)\n",
        "# ==============================================================================\n",
        "\n",
        "def limpieza_maestra(texto, estrategia=\"tfidf\"):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        texto: El string original.\n",
        "        estrategia: 'tfidf' (limpieza total) o 'beto' (limpieza estructural).\n",
        "    \"\"\"\n",
        "    if not isinstance(texto, str): return \"\"\n",
        "\n",
        "    # --- PASO 1: LIMPIEZA DE ARTEFACTOS ESPEC√çFICOS (LO QUE ME PEDISTE) ---\n",
        "\n",
        "    # 1.1 ELIMINACI√ìN PARA EMOEVENT (Literalmente las palabras \"USER\" y \"HASHTAG\")\n",
        "    # \\b asegura que sea la palabra completa. flags=re.I ignora may√∫sculas/min√∫sculas.\n",
        "    texto = re.sub(r'\\bUSER\\b', '', texto, flags=re.IGNORECASE)\n",
        "    texto = re.sub(r'\\bHASHTAG\\b', '', texto, flags=re.IGNORECASE)\n",
        "\n",
        "    # 1.2 ELIMINACI√ìN PARA INTERTASS (S√≠mbolos @ y # y sus contenidos)\n",
        "    texto = re.sub(r'@\\w+', '', texto)  # Elimina @usuario completo\n",
        "    texto = re.sub(r'#\\w+', '', texto)  # Elimina #tema completo\n",
        "    texto = re.sub(r'http\\S+|www\\.\\S+', '', texto) # Elimina URLs\n",
        "\n",
        "    # --- PASO 2: NORMALIZACI√ìN ---\n",
        "\n",
        "    # Eliminar espacios dobles generados por los borrados anteriores\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "    # Si es para BETO, paramos aqu√≠ (BETO necesita may√∫sculas y puntuaci√≥n para contexto)\n",
        "    if estrategia == 'beto':\n",
        "        return texto\n",
        "\n",
        "    # --- PASO 3: LIMPIEZA PROFUNDA (SOLO PARA TF-IDF / LOGISTIC REGRESSION) ---\n",
        "\n",
        "    # 3.1 Conversi√≥n a min√∫sculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # 3.2 Eliminaci√≥n de signos de puntuaci√≥n (.,!?) y N√∫meros\n",
        "    texto = texto.translate(str.maketrans(\"\", \"\", string.punctuation + \"¬°¬ø\"))\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "\n",
        "    # 3.3 Tokenizaci√≥n (Cortar en palabras)\n",
        "    tokens = word_tokenize(texto, language='spanish')\n",
        "\n",
        "    # 3.4 Eliminaci√≥n de Stopwords (el, la, de, que...)\n",
        "    tokens_limpios = [t for t in tokens if t not in stop_words_es and len(t) > 1]\n",
        "\n",
        "    return \" \".join(tokens_limpios)\n",
        "\n",
        "# ==============================================================================\n",
        "# B. APLICACI√ìN A LOS DOS DATASETS (SENTIMIENTOS Y EMOCIONES)\n",
        "# ==============================================================================\n",
        "print(\"\\n‚öôÔ∏è 1. Aplicando limpieza a ambos datasets...\")\n",
        "\n",
        "# Validamos que existan los dataframes de la Fase 1\n",
        "if 'df_sent' not in globals() or 'df_emo' not in globals():\n",
        "    raise ValueError(\"‚ö†Ô∏è Faltan los dataframes. Ejecuta la Fase 1 primero.\")\n",
        "\n",
        "# --- PROCESAMIENTO INTERTASS (SENTIMIENTOS) ---\n",
        "df_sent['text_tfidf'] = df_sent['content'].apply(lambda x: limpieza_maestra(x, estrategia='tfidf'))\n",
        "df_sent['text_beto'] = df_sent['content'].apply(lambda x: limpieza_maestra(x, estrategia='beto'))\n",
        "\n",
        "# --- PROCESAMIENTO EMOEVENT (EMOCIONES) ---\n",
        "df_emo['text_tfidf'] = df_emo['content'].apply(lambda x: limpieza_maestra(x, estrategia='tfidf'))\n",
        "df_emo['text_beto'] = df_emo['content'].apply(lambda x: limpieza_maestra(x, estrategia='beto'))\n",
        "\n",
        "print(\"   -> Limpieza completada.\")\n",
        "print(\"\\nüîç VERIFICACI√ìN (Revisa que no haya #, @, USER o HASHTAG):\")\n",
        "print(\"--- InterTASS (Original vs TF-IDF) ---\")\n",
        "print(df_sent[['content', 'text_tfidf']].head(2))\n",
        "print(\"\\n--- EmoEvent (Original vs TF-IDF) ---\")\n",
        "print(df_emo[['content', 'text_tfidf']].head(2))\n",
        "\n",
        "# ==============================================================================\n",
        "# C. DIVISI√ìN DE DATOS (TRAIN / TEST) PARA AMBOS\n",
        "# ==============================================================================\n",
        "print(\"\\n‚úÇÔ∏è 2. Dividiendo en Entrenamiento y Prueba (80/20)...\")\n",
        "\n",
        "# Mapas de etiquetas a n√∫meros\n",
        "labels_sent = sorted(df_sent['label'].unique())\n",
        "map_sent = {l: i for i, l in enumerate(labels_sent)}\n",
        "df_sent['label_id'] = df_sent['label'].map(map_sent)\n",
        "\n",
        "labels_emo = sorted(df_emo['label'].unique())\n",
        "map_emo = {l: i for i, l in enumerate(labels_emo)}\n",
        "df_emo['label_id'] = df_emo['label'].map(map_emo)\n",
        "\n",
        "# Split Sentimientos\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
        "    df_sent, df_sent['label_id'], test_size=0.2, stratify=df_sent['label_id'], random_state=42\n",
        ")\n",
        "\n",
        "# Split Emociones\n",
        "X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(\n",
        "    df_emo, df_emo['label_id'], test_size=0.2, stratify=df_emo['label_id'], random_state=42\n",
        ")\n",
        "\n",
        "print(f\"   - Train Sentimientos: {len(X_train_s)} | Train Emociones: {len(X_train_e)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# D. REPRESENTACI√ìN 1: TF-IDF (MODELO CL√ÅSICO)\n",
        "# ==============================================================================\n",
        "print(\"\\nüßÆ 3. Vectorizando con TF-IDF (Matriz Num√©rica)...\")\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "\n",
        "# Vectorizaci√≥n Sentimientos (Fit solo en train)\n",
        "X_train_tfidf_s = vectorizer.fit_transform(X_train_s['text_tfidf'])\n",
        "X_test_tfidf_s = vectorizer.transform(X_test_s['text_tfidf'])\n",
        "\n",
        "# Vectorizaci√≥n Emociones (Fit solo en train, reiniciando vectorizer para vocabulario nuevo)\n",
        "vectorizer_emo = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_tfidf_e = vectorizer_emo.fit_transform(X_train_e['text_tfidf'])\n",
        "X_test_tfidf_e = vectorizer_emo.transform(X_test_e['text_tfidf'])\n",
        "\n",
        "print(f\"   -> Matriz TF-IDF Sentimientos: {X_train_tfidf_s.shape}\")\n",
        "print(f\"   -> Matriz TF-IDF Emociones:    {X_train_tfidf_e.shape}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# E. REPRESENTACI√ìN 2: TOKENIZACI√ìN BETO (DEEP LEARNING)\n",
        "# ==============================================================================\n",
        "print(\"\\nü§ñ 4. Preparando Tokenizer BETO...\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
        "\n",
        "# Verificaci√≥n r√°pida\n",
        "ejemplo = X_train_e['text_beto'].iloc[0]\n",
        "print(f\"   -> Ejemplo Tokenizado (EmoEvent): {tokenizer.tokenize(ejemplo)[:10]}\")\n",
        "\n",
        "print(\"\\n‚úÖ FASE 2 COMPLETADA. Datasets limpios y listos.\")"
      ],
      "metadata": {
        "id": "PQQnbUssDxD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fase 3. Entrenamiento del modelo\n",
        "‚Ä¢\tModelo 1 (Sentimientos): entrenado con InterTASS\n",
        "o\tAlgoritmo: Logistic Regression Y BETO.\n",
        "\n",
        "o\tClases: P, N, NEU, NONE."
      ],
      "metadata": {
        "id": "IFbJ3tM0m_6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FASE 3: ENTRENAMIENTO DE MODELOS (SENTIMIENTOS Y EMOCIONES)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "# --- FIX: DESACTIVAR WANDB PARA QUE NO PIDA API KEY ---\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. VERIFICACI√ìN DE GPU ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚úÖ Usando dispositivo: {device}\")\n",
        "if device.type == 'cpu':\n",
        "    print(\"‚ö†Ô∏è ¬°ALERTA! Sigues en CPU. Activa T4 GPU para acelerar el proceso.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# [cite_start]PARTE A: MODELOS CL√ÅSICOS (Regresi√≥n Log√≠stica) [cite: 35]\n",
        "# ==============================================================================\n",
        "print(\"\\nüß† A. ENTRENANDO MODELOS CL√ÅSICOS (Regresi√≥n Log√≠stica)...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# --- A1. SENTIMIENTOS (INTERTASS) ---\n",
        "print(\"   -> 1. Entrenando Sentimientos (InterTASS)...\")\n",
        "# Usamos las variables _s creadas en la Fase 2\n",
        "clf_sent = LogisticRegression(\n",
        "    C=1.0,\n",
        "    solver='liblinear',\n",
        "    class_weight='balanced', # Crucial para datos desbalanceados\n",
        "    random_state=42,\n",
        "    max_iter=1000\n",
        ")\n",
        "clf_sent.fit(X_train_tfidf_s, y_train_s)\n",
        "\n",
        "# --- A2. EMOCIONES (EMOEVENT) ---\n",
        "print(\"   -> 2. Entrenando Emociones (EmoEvent)...\")\n",
        "# Usamos las variables _e creadas en la Fase 2\n",
        "clf_emo = LogisticRegression(\n",
        "    C=1.0,\n",
        "    solver='liblinear',\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    max_iter=1000\n",
        ")\n",
        "clf_emo.fit(X_train_tfidf_e, y_train_e)\n",
        "\n",
        "print(\"‚úÖ Modelos Cl√°sicos listos.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PARTE B: PREPARACI√ìN DE DATOS PARA BETO (DEEP LEARNING)\n",
        "# ==============================================================================\n",
        "print(\"\\n‚öôÔ∏è B. PREPARANDO DATASETS PARA BETO (HUGGING FACE)...\")\n",
        "\n",
        "# Funci√≥n de tokenizaci√≥n (Reutilizable)\n",
        "def tokenizar_para_bert(examples):\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "# --- B1. SENTIMIENTOS ---\n",
        "# Convertimos Pandas DataFrame a Dataset de Hugging Face\n",
        "ds_train_s = Dataset.from_pandas(pd.DataFrame({'text': X_train_s['text_beto'], 'label': y_train_s}))\n",
        "ds_test_s = Dataset.from_pandas(pd.DataFrame({'text': X_test_s['text_beto'], 'label': y_test_s}))\n",
        "# Tokenizamos\n",
        "tokenized_train_s = ds_train_s.map(tokenizar_para_bert, batched=True)\n",
        "tokenized_test_s = ds_test_s.map(tokenizar_para_bert, batched=True)\n",
        "\n",
        "# --- B2. EMOCIONES ---\n",
        "ds_train_e = Dataset.from_pandas(pd.DataFrame({'text': X_train_e['text_beto'], 'label': y_train_e}))\n",
        "ds_test_e = Dataset.from_pandas(pd.DataFrame({'text': X_test_e['text_beto'], 'label': y_test_e}))\n",
        "# Tokenizamos\n",
        "tokenized_train_e = ds_train_e.map(tokenizar_para_bert, batched=True)\n",
        "tokenized_test_e = ds_test_e.map(tokenizar_para_bert, batched=True)\n",
        "\n",
        "# Funci√≥n de m√©tricas (com√∫n para ambos)\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted') # Weighted por el desbalance\n",
        "    return {'accuracy': acc, 'f1': f1}\n",
        "\n",
        "# ==============================================================================\n",
        "# PARTE C: ENTRENAMIENTO DE BETO (SENTIMIENTOS)\n",
        "# ==============================================================================\n",
        "print(\"\\nüî• C. ENTRENAMIENTO BETO: SENTIMIENTOS (INTERTASS)...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Cargar BETO limpio para 4 clases (P, N, NEU, NONE)\n",
        "model_beto_s = BertForSequenceClassification.from_pretrained(\n",
        "    \"dccuchile/bert-base-spanish-wwm-cased\",\n",
        "    num_labels=4\n",
        ").to(device)\n",
        "\n",
        "args_s = TrainingArguments(\n",
        "    output_dir='./results_sent',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\" # Evita API Key\n",
        ")\n",
        "\n",
        "trainer_s = Trainer(\n",
        "    model=model_beto_s,\n",
        "    args=args_s,\n",
        "    train_dataset=tokenized_train_s,\n",
        "    eval_dataset=tokenized_test_s,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_s.train()\n",
        "trainer_s.save_model(\"./modelo_beto_sentimientos\") # Guardamos\n",
        "print(\"‚úÖ Modelo Sentimientos Guardado.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PARTE D: ENTRENAMIENTO DE BETO (EMOCIONES)\n",
        "# ==============================================================================\n",
        "print(\"\\nüî• D. ENTRENAMIENTO BETO: EMOCIONES (EMOEVENT)...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Detectamos cu√°ntas emociones hay din√°micamente (deber√≠an ser 7)\n",
        "num_labels_emo = len(map_emo)\n",
        "\n",
        "# Cargar BETO limpio para 7 clases\n",
        "model_beto_e = BertForSequenceClassification.from_pretrained(\n",
        "    \"dccuchile/bert-base-spanish-wwm-cased\",\n",
        "    num_labels=num_labels_emo\n",
        ").to(device)\n",
        "\n",
        "args_e = TrainingArguments(\n",
        "    output_dir='./results_emo',\n",
        "    num_train_epochs=4, # Emociones es m√°s dif√≠cil, damos una √©poca extra\n",
        "    per_device_train_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_e = Trainer(\n",
        "    model=model_beto_e,\n",
        "    args=args_e,\n",
        "    train_dataset=tokenized_train_e,\n",
        "    eval_dataset=tokenized_test_e,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_e.train()\n",
        "trainer_e.save_model(\"./modelo_beto_emociones\") # Guardamos\n",
        "print(\"‚úÖ Modelo Emociones Guardado.\")\n",
        "\n",
        "print(\"\\nüéâ FASE 3 COMPLETADA: 4 MODELOS ENTRENADOS Y LISTOS.\")"
      ],
      "metadata": {
        "id": "sE0_sbx6HBYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fase 4. Evaluaci√≥n y comparaci√≥n\n",
        "\n",
        "‚Ä¢\tM√©tricas:\n",
        "\n",
        "o\tAccuracy (precisi√≥n global)\n",
        "\n",
        "o\tRecall (sensibilidad por clase)\n",
        "\n",
        "o\tF1-Score (balance entre precisi√≥n y recall)\n",
        "\n",
        "‚Ä¢\tVisualizaci√≥n con matriz de confusi√≥n.\n",
        "\n",
        "‚Ä¢\tIdentificaci√≥n de palabras m√°s frecuentes por categor√≠a.\n"
      ],
      "metadata": {
        "id": "T5oB79xnPRK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FASE 4: EVALUACI√ìN INTEGRAL, COMPARACI√ìN Y AN√ÅLISIS DE PALABRAS CLAVE\n",
        "# ==============================================================================\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Configuraci√≥n de tama√±o de gr√°ficos\n",
        "plt.rcParams['figure.figsize'] = (20, 12)\n",
        "\n",
        "print(\"üìä INICIANDO PROCESO DE EVALUACI√ìN FINAL...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. GENERACI√ìN DE PREDICCIONES (INFERENCIA MASIVA EN SET DE PRUEBA)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- A. SENTIMIENTOS (INTERTASS) ---\n",
        "print(\"   -> Generando predicciones para SENTIMIENTOS...\")\n",
        "# 1. Cl√°sico (Regresi√≥n Log√≠stica)\n",
        "y_pred_cl_s = clf_sent.predict(X_test_tfidf_s)\n",
        "\n",
        "# 2. Deep Learning (BETO)\n",
        "# El trainer devuelve 'logits' (probabilidades crudas), usamos argmax para elegir la clase ganadora\n",
        "raw_pred_s = trainer_s.predict(tokenized_test_s)\n",
        "y_pred_dl_s = np.argmax(raw_pred_s.predictions, axis=-1)\n",
        "y_true_s = raw_pred_s.label_ids # Etiquetas reales\n",
        "\n",
        "# --- B. EMOCIONES (EMOEVENT) ---\n",
        "print(\"   -> Generando predicciones para EMOCIONES...\")\n",
        "# 1. Cl√°sico (Regresi√≥n Log√≠stica)\n",
        "y_pred_cl_e = clf_emo.predict(X_test_tfidf_e)\n",
        "\n",
        "# 2. Deep Learning (BETO)\n",
        "raw_pred_e = trainer_e.predict(tokenized_test_e)\n",
        "y_pred_dl_e = np.argmax(raw_pred_e.predictions, axis=-1)\n",
        "y_true_e = raw_pred_e.label_ids # Etiquetas reales\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. DEFINICI√ìN DE ETIQUETAS LEGIBLES\n",
        "# ==============================================================================\n",
        "# Recuperamos los nombres de las clases (N=0, NEU=1, P=2, NONE=3)\n",
        "nombres_sent = ['Negativo', 'Neutral', 'Positivo', 'No Asignado']\n",
        "\n",
        "# Recuperamos nombres de emociones (ordenados por su ID num√©rico 0,1,2...)\n",
        "# Usamos el mapa inverso que creamos en Fase 2 (map_emo)\n",
        "nombres_emo = [k for k, v in sorted(map_emo.items(), key=lambda item: item[1])]\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. REPORTES T√âCNICOS (M√âTRICAS DETALLADAS)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"‚ñà\"*40 + \"\\n   RESULTADOS: SENTIMIENTOS (INTERTASS)\\n\" + \"‚ñà\"*40)\n",
        "print(\"\\n--- MODELO CL√ÅSICO (BASELINE) ---\")\n",
        "print(classification_report(y_test_s, y_pred_cl_s, target_names=nombres_sent, digits=4))\n",
        "\n",
        "print(\"\\n--- MODELO DEEP LEARNING (BETO) ---\")\n",
        "print(classification_report(y_true_s, y_pred_dl_s, target_names=nombres_sent, digits=4))\n",
        "\n",
        "print(\"\\n\" + \"‚ñà\"*40 + \"\\n   RESULTADOS: EMOCIONES (EMOEVENT)\\n\" + \"‚ñà\"*40)\n",
        "print(\"\\n--- MODELO CL√ÅSICO (BASELINE) ---\")\n",
        "print(classification_report(y_test_e, y_pred_cl_e, target_names=nombres_emo, digits=4))\n",
        "\n",
        "print(\"\\n--- MODELO DEEP LEARNING (BETO) ---\")\n",
        "print(classification_report(y_true_e, y_pred_dl_e, target_names=nombres_emo, digits=4))\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. VISUALIZACI√ìN: MATRICES DE CONFUSI√ìN (GRID 2x2)\n",
        "# ==============================================================================\n",
        "print(\"\\nüé® GENERANDO MATRICES DE CONFUSI√ìN COMPARATIVAS...\")\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "# --- FILA 1: SENTIMIENTOS ---\n",
        "# Cl√°sico\n",
        "cm_s_cl = confusion_matrix(y_test_s, y_pred_cl_s)\n",
        "sns.heatmap(cm_s_cl, annot=True, fmt='d', cmap='Blues', ax=ax[0,0],\n",
        "            xticklabels=nombres_sent, yticklabels=nombres_sent)\n",
        "ax[0,0].set_title('SENTIMIENTOS: Modelo Cl√°sico', fontsize=14, fontweight='bold')\n",
        "ax[0,0].set_ylabel('Realidad')\n",
        "\n",
        "# BETO\n",
        "cm_s_dl = confusion_matrix(y_true_s, y_pred_dl_s)\n",
        "sns.heatmap(cm_s_dl, annot=True, fmt='d', cmap='Greens', ax=ax[0,1],\n",
        "            xticklabels=nombres_sent, yticklabels=nombres_sent)\n",
        "ax[0,1].set_title('SENTIMIENTOS: Modelo BETO (GPU)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# --- FILA 2: EMOCIONES ---\n",
        "# Cl√°sico\n",
        "cm_e_cl = confusion_matrix(y_test_e, y_pred_cl_e)\n",
        "sns.heatmap(cm_e_cl, annot=True, fmt='d', cmap='Oranges', ax=ax[1,0],\n",
        "            xticklabels=nombres_emo, yticklabels=nombres_emo)\n",
        "ax[1,0].set_title('EMOCIONES: Modelo Cl√°sico', fontsize=14, fontweight='bold')\n",
        "ax[1,0].set_ylabel('Realidad')\n",
        "ax[1,0].set_xlabel('Predicci√≥n')\n",
        "\n",
        "# BETO\n",
        "cm_e_dl = confusion_matrix(y_true_e, y_pred_dl_e)\n",
        "sns.heatmap(cm_e_dl, annot=True, fmt='d', cmap='Purples', ax=ax[1,1],\n",
        "            xticklabels=nombres_emo, yticklabels=nombres_emo)\n",
        "ax[1,1].set_title('EMOCIONES: Modelo BETO (GPU)', fontsize=14, fontweight='bold')\n",
        "ax[1,1].set_xlabel('Predicci√≥n')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. INTERPRETABILIDAD: PALABRAS CLAVE (DEL MODELO CL√ÅSICO)\n",
        "# ==============================================================================\n",
        "# Analizamos los coeficientes de la Regresi√≥n Log√≠stica para entender qu√© palabras\n",
        "# pesan m√°s para cada clase.\n",
        "\n",
        "def mostrar_top_palabras(modelo, vectorizador, etiquetas, titulo):\n",
        "    print(f\"\\nüîë PALABRAS CLAVE: {titulo}\")\n",
        "    print(\"-\" * 80)\n",
        "    feature_names = np.array(vectorizador.get_feature_names_out())\n",
        "    coefs = modelo.coef_\n",
        "\n",
        "    for idx, nombre in enumerate(etiquetas):\n",
        "        # Top 10 palabras positivas (que m√°s empujan hacia esta clase)\n",
        "        top_indices = np.argsort(coefs[idx])[-8:][::-1]\n",
        "        top_words = feature_names[top_indices]\n",
        "        print(f\"[{nombre}]: {', '.join(top_words)}\")\n",
        "\n",
        "# Ejecutar an√°lisis\n",
        "mostrar_top_palabras(clf_sent, vectorizer, nombres_sent, \"SENTIMIENTOS (InterTASS)\")\n",
        "mostrar_top_palabras(clf_emo, vectorizer_emo, nombres_emo, \"EMOCIONES (EmoEvent)\")\n",
        "\n",
        "print(\"\\n‚úÖ FASE 4 COMPLETADA.\")"
      ],
      "metadata": {
        "id": "keWlgF6fPW48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fase 5. Implementaci√≥n pr√°ctica\n",
        "\n",
        "‚Ä¢\tCrear una interfaz simple (o script) donde el usuario ingrese un texto y el sistema:\n",
        "\n",
        "1.\tClasifique su sentimiento.\n",
        "\n",
        "2.\tMuestre ambos resultados de manera conjunta.\n"
      ],
      "metadata": {
        "id": "yRH8y5c1QIbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FASE 5 - PARTE A: CONFIGURACI√ìN Y PROCESAMIENTO MASIVO (SAMPLE)\n",
        "# ==============================================================================\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xmltodict\n",
        "import os\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# 1. CONFIGURACI√ìN DE ETIQUETAS\n",
        "# ------------------------------------------------------------------------------\n",
        "# [cite_start]Mapa Sentimientos (Fijo) [cite: 1766-1770]\n",
        "mapa_sent_inv = {0: 'NEGATIVO (N)', 1: 'NEUTRAL (NEU)', 2: 'POSITIVO (P)', 3: 'NO ASIGNADO (NONE)'}\n",
        "\n",
        "# Mapa Emociones (Din√°mico, recuperado de la Fase 2)\n",
        "# Invertimos el diccionario map_emo: {0: 'Alegr√≠a', 1: 'Asco'...}\n",
        "if 'map_emo' in globals():\n",
        "    mapa_emo_inv = {v: k for k, v in map_emo.items()}\n",
        "else:\n",
        "    # Fallback por si se perdi√≥ la variable en memoria\n",
        "    print(\"‚ö†Ô∏è Reconstruyendo mapa de emociones por defecto...\")\n",
        "    emociones_default = ['Alegr√≠a', 'Asco', 'Ira', 'Miedo', 'Neutral', 'Sorpresa', 'Tristeza']\n",
        "    mapa_emo_inv = {i: e for i, e in enumerate(emociones_default)}\n",
        "\n",
        "# 2. RECARGA DE MODELOS BETO (DESDE EL DISCO A LA GPU)\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"üîß Cargando modelos neuronales desde el disco...\")\n",
        "\n",
        "try:\n",
        "    # Cargar Modelo Sentimientos\n",
        "    model_beto_s_final = BertForSequenceClassification.from_pretrained(\"./modelo_beto_sentimientos\").to(device)\n",
        "    model_beto_s_final.eval() # Modo evaluaci√≥n (congela pesos)\n",
        "    print(\"   -> ‚úÖ Modelo Sentimientos cargado.\")\n",
        "\n",
        "    # Cargar Modelo Emociones\n",
        "    model_beto_e_final = BertForSequenceClassification.from_pretrained(\"./modelo_beto_emociones\").to(device)\n",
        "    model_beto_e_final.eval()\n",
        "    print(\"   -> ‚úÖ Modelo Emociones cargado.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error cargando modelos: {e}\")\n",
        "    print(\"Aseg√∫rate de haber ejecutado la Fase 3 completamente.\")\n",
        "\n",
        "# 3. FUNCI√ìN MAESTRA DE PREDICCI√ìN\n",
        "# ------------------------------------------------------------------------------\n",
        "def predecir_todo(texto):\n",
        "    \"\"\"\n",
        "    Recibe texto y devuelve diccionario con predicciones de Sentimiento y Emoci√≥n.\n",
        "    \"\"\"\n",
        "    res = {}\n",
        "\n",
        "    # --- Preprocesamiento ---\n",
        "    clean_beto = limpieza_para_beto(texto) # Usamos la funci√≥n de la Fase 2\n",
        "\n",
        "    # Tokenizar para BETO\n",
        "    inputs = tokenizer(clean_beto, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
        "\n",
        "    # --- Inferencia Sentimientos ---\n",
        "    with torch.no_grad():\n",
        "        logits_s = model_beto_s_final(**inputs).logits\n",
        "    idx_s = logits_s.argmax().item()\n",
        "    conf_s = torch.nn.functional.softmax(logits_s, dim=-1)[0][idx_s].item()\n",
        "    res['Sentimiento'] = mapa_sent_inv[idx_s]\n",
        "    res['Conf_Sent'] = conf_s\n",
        "\n",
        "    # --- Inferencia Emociones ---\n",
        "    with torch.no_grad():\n",
        "        logits_e = model_beto_e_final(**inputs).logits\n",
        "    idx_e = logits_e.argmax().item()\n",
        "    conf_e = torch.nn.functional.softmax(logits_e, dim=-1)[0][idx_e].item()\n",
        "    res['Emoci√≥n'] = mapa_emo_inv[idx_e]\n",
        "    res['Conf_Emo'] = conf_e\n",
        "\n",
        "    return res\n",
        "\n",
        "# 4. PROCESAMIENTO DEL ARCHIVO SAMPLE\n",
        "# ------------------------------------------------------------------------------\n",
        "ruta_sample = '/content/data/general-tweets-sample.xml'\n",
        "print(f\"\\nüìÇ Analizando archivo de muestras: {ruta_sample}...\")\n",
        "\n",
        "if os.path.exists(ruta_sample):\n",
        "    with open(ruta_sample, 'r', encoding='utf-8') as f:\n",
        "        data_sample = xmltodict.parse(f.read())\n",
        "\n",
        "    try:\n",
        "        # Extracci√≥n de tweets\n",
        "        if 'tweets' in data_sample: list_tw = data_sample['tweets']['tweet']\n",
        "        else: list_tw = data_sample[list(data_sample.keys())[0]]['tweet']\n",
        "\n",
        "        tabla_resultados = []\n",
        "\n",
        "        print(f\"   -> Procesando {len(list_tw)} tweets con IA...\")\n",
        "\n",
        "        for t in list_tw:\n",
        "            contenido = t['content']\n",
        "            pred = predecir_todo(contenido)\n",
        "\n",
        "            # Formato solicitado en tu documento\n",
        "            tabla_resultados.append({\n",
        "                'Texto de entrada': contenido,\n",
        "                'Sentimiento': f\"{pred['Sentimiento']}\",\n",
        "                'Emoci√≥n': f\"{pred['Emoci√≥n']}\",\n",
        "                'Confianza': f\"S:{pred['Conf_Sent']:.2f} | E:{pred['Conf_Emo']:.2f}\"\n",
        "            })\n",
        "\n",
        "        # Visualizaci√≥n\n",
        "        df_res = pd.DataFrame(tabla_resultados)\n",
        "        pd.set_option('display.max_colwidth', 100)\n",
        "        print(\"\\nüìä TABLA DE RESULTADOS (SAMPLE):\")\n",
        "        display(df_res)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error procesando XML: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Archivo sample no encontrado.\")"
      ],
      "metadata": {
        "id": "O3z-4ZAgQdUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INPUT DE USUARIO"
      ],
      "metadata": {
        "id": "ocyQuB-5c6Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FASE 5 - PARTE B: LABORATORIO INTERACTIVO\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"‚ñà\"*70)\n",
        "print(\"üß™ LABORATORIO DE IA: CLASIFICADOR MULTI-TAREA\")\n",
        "print(\"   -> Detecta Sentimiento (P, N, NEU) y Emoci√≥n (Ira, Alegr√≠a, etc.)\")\n",
        "print(\"   -> Escribe 'salir' para terminar.\")\n",
        "print(\"‚ñà\"*70)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        txt = input(\"\\nüìù Tu frase: \")\n",
        "\n",
        "        if txt.lower() in ['salir', 'exit', 'fin']:\n",
        "            print(\"üëã ¬°Hasta luego!\")\n",
        "            break\n",
        "\n",
        "        if not txt.strip(): continue\n",
        "\n",
        "        # Predicci√≥n en tiempo real\n",
        "        resultado = predecir_todo(txt)\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "        # Formato visual tipo \"Dashboard\"\n",
        "        print(f\"üß† AN√ÅLISIS DE TEXTO:\")\n",
        "        print(f\"   \\\"{txt}\\\"\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # L√≥gica de colores simple (usando iconos)\n",
        "        icono_sent = \"üü¢\" if \"POSITIVO\" in resultado['Sentimiento'] else \"üî¥\" if \"NEGATIVO\" in resultado['Sentimiento'] else \"‚ö™\"\n",
        "\n",
        "        print(f\"{icono_sent} SENTIMIENTO:  {resultado['Sentimiento']}\")\n",
        "        print(f\"   (Confianza: {resultado['Conf_Sent']:.2%})\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        print(f\"‚ù§Ô∏è EMOCI√ìN:      {resultado['Emoci√≥n']}\")\n",
        "        print(f\"   (Confianza: {resultado['Conf_Emo']:.2%})\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en la predicci√≥n: {e}\")"
      ],
      "metadata": {
        "id": "Lgj4ojodQhlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SCRIPT DE RESCATE: CARGA DE MODELOS Y DATOS DESDE DISCO\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Configuraci√≥n de Entorno (Re-importar y desactivar logging)\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from datasets import Dataset # Necesaria para la funci√≥n de tokenizaci√≥n\n",
        "\n",
        "# Re-verificar GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"‚úÖ Dispositivo recuperado: {device}\")\n",
        "\n",
        "# --- RUTAS DE ARCHIVOS (Debe coincidir con la Fase 1) ---\n",
        "ruta_intertass = '/content/data/general-train-tagged.xml'\n",
        "ruta_emoevent = '/content/data/emoevent_es.csv'\n",
        "# --- RUTAS DE MODELOS GUARDADOS ---\n",
        "ruta_beto_s = \"./modelo_beto_sentimientos\"\n",
        "ruta_beto_e = \"./modelo_beto_emociones\"\n",
        "\n",
        "# ==============================================================================\n",
        "# PASO 1: RECARGAR DATOS CRUDOS Y EJECUTAR FASE 1 & 2 COMPLETAS\n",
        "#         (Esto es r√°pido y necesario para recrear TF-IDF)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- RECARGA DE FUNCIONES DE LA FASE 1 (ASUMIMOS QUE YA EST√ÅN EN LA SESI√ìN) ---\n",
        "# Si las funciones cargar_sentimientos_xml y cargar_emociones_csv no est√°n definidas,\n",
        "# copia y pega de nuevo la FASE 1 MAESTRA completa antes de este bloque.\n",
        "\n",
        "# 1. Ejecutar Fase 1 (Carga de datos)\n",
        "df_sent, _ = cargar_sentimientos_xml(ruta_intertass)\n",
        "df_emo, _ = cargar_emociones_csv(ruta_emoevent)\n",
        "\n",
        "# 2. Recrear funciones de limpieza (Asumimos las definiciones de la Fase 2 est√°n en memoria)\n",
        "# Si las funciones limpieza_para_tfidf y limpieza_para_beto no est√°n definidas,\n",
        "# copia y pega la FASE 2, PARTE A, antes de este bloque.\n",
        "\n",
        "# 3. Aplicar Limpieza y Split\n",
        "df_sent['text_tfidf'] = df_sent['content'].apply(lambda x: limpieza_maestra(x, estrategia='tfidf'))\n",
        "df_sent['text_beto'] = df_sent['content'].apply(lambda x: limpieza_maestra(x, estrategia='beto'))\n",
        "df_emo['text_tfidf'] = df_emo['content'].apply(lambda x: limpieza_maestra(x, estrategia='tfidf'))\n",
        "df_emo['text_beto'] = df_emo['content'].apply(lambda x: limpieza_maestra(x, estrategia='beto'))\n",
        "\n",
        "# --- RECREAR MAPAS Y SPLIT ---\n",
        "labels_sent = sorted(df_sent['label'].unique())\n",
        "map_sent = {l: i for i, l in enumerate(labels_sent)}\n",
        "df_sent['label_id'] = df_sent['label'].map(map_sent)\n",
        "labels_emo = sorted(df_emo['label'].unique())\n",
        "map_emo = {l: i for i, l in enumerate(labels_emo)}\n",
        "df_emo['label_id'] = df_emo['label'].map(map_emo)\n",
        "\n",
        "# Split (necesario para recrear X_train, X_test, y_train, y_test)\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(df_sent, df_sent['label_id'], test_size=0.2, stratify=df_sent['label_id'], random_state=42)\n",
        "X_train_e, X_test_e, y_train_e, y_test_e = train_test_split(df_emo, df_emo['label_id'], test_size=0.2, stratify=df_emo['label_id'], random_state=42)\n",
        "\n",
        "# 4. Recrear Vectorizadores (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "vectorizer = tfidf_vectorizer.fit(X_train_s['text_tfidf'])\n",
        "X_train_tfidf_s = vectorizer.transform(X_train_s['text_tfidf'])\n",
        "vectorizer_emo = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "vectorizer_emo.fit(X_train_e['text_tfidf'])\n",
        "X_train_tfidf_e = vectorizer_emo.transform(X_train_e['text_tfidf']) # Necesario para re-entrenar cl√°sico\n",
        "\n",
        "# 5. Cargar Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
        "\n",
        "print(\"\\n‚úÖ DATOS RECREADOS Y VECTORIZADORES FIT.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PASO 2: RE-ENTRENAR CL√ÅSICOS (R√°pido)\n",
        "# ==============================================================================\n",
        "print(\"\\nüîÑ Re-entrenando Modelos Cl√°sicos (2 segundos)...\")\n",
        "\n",
        "clf_sent = LogisticRegression(C=1.0, solver='liblinear', class_weight='balanced', random_state=42, max_iter=1000)\n",
        "clf_sent.fit(X_train_tfidf_s, y_train_s)\n",
        "\n",
        "clf_emo = LogisticRegression(C=1.0, solver='liblinear', class_weight='balanced', random_state=42, max_iter=1000)\n",
        "clf_emo.fit(X_train_tfidf_e, y_train_e)\n",
        "\n",
        "print(\"‚úÖ Modelos Cl√°sicos recuperados.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# PASO 3: RECARGAR MODELOS BETO (Time Saver)\n",
        "# ==============================================================================\n",
        "print(\"\\nüíæ Recargando modelos BETO (Sentimientos y Emociones) desde disco...\")\n",
        "\n",
        "# Cargar BETO Sentimientos\n",
        "model_beto_s = BertForSequenceClassification.from_pretrained(ruta_beto_s).to(device)\n",
        "\n",
        "# Cargar BETO Emociones\n",
        "model_beto_e = BertForSequenceClassification.from_pretrained(ruta_beto_e).to(device)\n",
        "\n",
        "# Los Trainers originales ya no existen, pero los modelos est√°n listos para la Fase 5.\n",
        "# Si necesitas volver a evaluar (Fase 4), tendr√°s que re-crear el objeto Trainer.\n",
        "print(\"‚úÖ MODELOS BETO RECARGADOS Y LISTOS PARA INFERENCIA (FASE 5).\")"
      ],
      "metadata": {
        "id": "DQfzaxfbD5-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}